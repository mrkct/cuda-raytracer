{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Relazione.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO8+pH3h72ym0Fnxx1uoaSc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrkct/cuda-raytracer/blob/master/Relazione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxCbi45Ce3zb"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!rm -rf cuda-raytracer\n",
        "!git clone https://github.com/mrkct/cuda-raytracer.git\n",
        "%cd cuda-raytracer\n",
        "!make all"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raytracer accelerato via *CUDA*"
      ],
      "metadata": {
        "id": "klrLIpbMuhx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il raytracing è una tecnica utilizzata in computer grafica per il rendering di immagini realistiche. L'idea alla base del raytracing è quella di proiettare dei raggi di luce a partire dall'occhio dello spettatore e, simulando il comportamento dei raggi che interagiscono con gli oggetti nella scena, determinare il colore di un pixel dell'immagine finale. Questa tecnica è raramente utilizzata nella computer grafica in tempo reale a causa della sua pesantezza computazionale perchè richiede di simulare un numero significativo di raggi; almeno uno per ogni pixel dell'immagine finale.\n",
        "Il calcolo di questi raggi è completamente indipendente tra di loro; questo rende il raytracing rende un candidato ideale alla parallelizzazione.\n",
        "\n",
        "Il raytracer implementato in questo report è basato su quello  del libro *Raytracing in One Weekend*[1], dove viene implementato in una versione single-thread su CPU. In aggiunta alle feature del libro ho implementato anche il texturing degli oggetti e l'export delle immagini in formato `png`; il video qui sotto è stato prodotto dal mio raytracer generando le immagini dei singoli fotogrammi ed assemblandoli in un video utilizzando `ffmpeg`.**bold text**"
      ],
      "metadata": {
        "id": "YNfLfMEhe7OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "Video(\"demo.mp4\")"
      ],
      "metadata": {
        "id": "kGdpexTRmkhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo per il raytracing"
      ],
      "metadata": {
        "id": "44oxJdXTum5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alla base del raytracing sta il concetto di *raggio*; un raggio è semplicemente una retta in uno spazio tridimensionale, dunque possiamo esprimere i punti su di essa con la semplice formula $P(t) = A + B \\cdot t$.\n",
        "\n",
        "Definiremo questi raggi tramite due punti per cui passano: il primo punto è l'occhio dello spettatore (una costante per tutti i raggi iniziali), l'altro è un punto sulla *vista*. Questa vista è una proiezione nello spazio 3D dell'immagine finale che vorremo generare: dato un punto di coordinate $(x, y)$ nell'immagine finale allora il suo corrispondente nello spazio 3D sul piano sarà dato da\n",
        "\n",
        "$$\n",
        "  \\frac{x}{ImageWidth} \\cdot ViewWidth \\cdot H + \\frac{y}{ImageHeight} \\cdot ViewHeight \\cdot V + LowerLeftCorner\n",
        "$$\n",
        "\n",
        "In questa formula $H$ e $V$ sono due vettori unitari che puntano nelle direzioni per muoversi orizzontalmente e verticalmente sul piano.\n",
        "\n",
        "La dimensione e posizione del piano rispetto alla telecamera ha effeti interessanti sull'immagine finale: tenendo fissa la dimensione del piano ma allontanandolo dallo spettatore provoca un avvicinamento sempre maggiore dei raggi proiettati sul piano che finiscono per concentrarsi un'area più piccola ma in modo più denso: questo è come funziona lo zoom di una telecamera. La distanza di questo piano dalla telecamera è un parametro detto *lunghezza focale*.\n",
        "\n",
        "    **IMMAGINI DEL CAMBIO DI FOCAL LENGTH**\n",
        "\n",
        "Invece, tenendo fissa la distanza tra telecamera e vista ma variando la dimensione di quest'ultima blabla.\n",
        "\n",
        "Un raytracer, per ogni pixel dell'immagine, proietta un raggio con associato un colore iniziale ed osserva il colore finale del raggio dopo che questo va a collidere con eventuali oggetti nella sua traiettoria dalla telecamera al piano. Il materiale di cui sono fatti gli oggetti con cui un raggio va a collidere determina il modo in cui il colore del raggio viene modificato: per esempio un materiale opaco assorbe una parte di luminosità del raggio.\n",
        "\n",
        "Il seguente pseudocodice mostra il \"cuore\" di un raytracer, seppur nasconda alcuni aspetti importanti come un modo per trovare l'intersezione tra raggio e oggetti oppure la *funzione di dispersione di colore* dei materiali: il motivo è semplicemente che questi aspetti dipendono dalla scena che vogliamo renderizzare.\n",
        "\n",
        "```c\n",
        "for (int x = 0; x < ImageWidth; x++) {\n",
        "  for (int y = 0; y < ImageHeight; y++) {\n",
        "    Sia `p` il punto sul piano corrispondente al pixel (x, y)\n",
        "    Sia `r` una retta che passa per i punti `CameraPosition` e `p`\n",
        "    Sia `o` il primo oggetto che interseca la retta `r` partendo da `CameraPosition`\n",
        "      in avanti\n",
        "    \n",
        "    if (la retta `r` non ha intersecato alcun oggetto) then {\n",
        "      Il pixel (x, y) ha colore dello sfondo\n",
        "    } else {\n",
        "      Sia `m` il materiale di cui è composto `o`\n",
        "      Sia `c` il colore ottenuto applicando la funzione di dispersione di colore\n",
        "        del materiale `m` al colore dello sfondo\n",
        "      Il pixel (x, y) ha colore `c`\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "2S7JOdskm-m9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcolo dell'intersezione tra un raggio ed una sfera"
      ],
      "metadata": {
        "id": "bunnv8KqvI1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per trovare se la retta `r` interseca con qualche oggetto una possibile soluzione è quella di iterare su tutti gli oggetti della scena e, utilizzando qualche formula geometrica, calcolare il punto di intersezione tra l'oggetto e la retta (se esiste). L'oggetto con il punto di intersezione con distanza minore dalla telecamera è quello il cui materiale deve essere usato per determinare il colore del raggio.\n",
        "\n",
        "Per esempio se volessimo calcolare l'intersezione tra una retta ed una sfera di raggio $r$ centrata in $C$ potremmo osservare che i punti su una sfera sono descritti dall'equazione:\n",
        "\n",
        "$$\n",
        "  (x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2 = r^2\n",
        "$$\n",
        "\n",
        "Che possiamo scrivere anche in questo modo:\n",
        "\n",
        "$$\n",
        "  r^2 = (P - C) \\cdot (P - C)\n",
        "$$\n",
        "\n",
        "I punti del nostro raggio sono descritti dalla funzione $P(t)=A + t \\cdot b$, vogliamo dunque trovare se esistono dei punti su $P(t)$ che soddisfano anche l'equazione. Sostituiamo dunque a $P$ l'equazione della nostra retta e, dopo un po' di semplificazioni matematiche, otteniamo una equazione di secondo grado:\n",
        "\n",
        "$$\n",
        "  (A + t \\cdot B - C) \\cdot (A + t \\cdot B - C) = r^2\n",
        "$$\n",
        "$$\n",
        "  t \\cdot 2b^2 + t \\cdot 2b (A-C) + (A-C) \\cdot (A-C) - r^2 = 0\n",
        "$$\n",
        "\n",
        "In questa equazione l'incognita $t$ sono i punti di intersezione, è possibile risolverla con la classica formula $\\frac{-b +- \\sqrt{b^2 - 4ac}}{2a}$. In base al numero di soluzioni trovate possiamo capire se il raggio non interseca la sfera (zero soluzioni trovate, discriminante negativo), se la retta passa tangente per un solo punto (una sola soluzione, discriminante uguale a zero) oppure se la retta attraversa la sfera e dunque ci sono due punti di intersezione; in quest'ultimo caso sceglieremo la soluzione con distanza minore dalla telecamera.\n",
        "\n",
        "    Immagine delle rette che attraversano il cerchio"
      ],
      "metadata": {
        "id": "sy816-RSvI4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcolo della funzione di dispersione del colore di un materiale opaco"
      ],
      "metadata": {
        "id": "GMIehwlhvVVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un materiale opaco prende il suo colore mischiando il suo colore innato (detto *albedo*) a quello ottenuto dai materiali riflessi su di esso. Il materiale opaco che andiamo a simulare è detto *Lambertiano* e la sua riflettenza è pari in qualunque punto e direzione. Un modo molto semplice per simulare il colore di un raggio che interseca un materiale Lambertiano è quello di prendere l'albedo del materiale (rappresentato come un vettore a 3 dimensioni di valori $RBG$ tra $[0, 1]$) e di moltiplicarlo componente per componente con il colore di un raggio proiettato dal punto di intersezione e il vettore normale della superficie.\n",
        "\n",
        "    Immagine della sfera e dei raggi che rimbalzano tra sfera e pavimento\n",
        "\n",
        "Per esempio, se determiniamo che il nostro raggio colpisce una sfera nella posizione $P$ e questa sfera è composta di un materiale Lambertiano con albedo rosso $(0.7, 0, 0)$, allora per calcolare il colore del raggio dovremo prima determinare ricorsivamente il colore di un raggio che parte da $P$ ed ha come direzione il vettore normale alla curva e poi moltiplicare questo colore per l'albedo del materiale, in questo caso $(0.7, 0, 0)$.\n",
        "\n",
        "Per determinare il vettore normale alla superficie di una sfera basta sottrarre al punto $P$ il punto origine della sfera e normalizzare il vettore."
      ],
      "metadata": {
        "id": "yc5w4tmfvViF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accelerazione tramite GPU"
      ],
      "metadata": {
        "id": "HdCvuGlbu5R6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per accelerare via CUDA questo processo ho diviso l'immagine finale in blocchi quadrati di lato $8$, ogni thread si occupa di un singolo pixel dell'immagine finale e dunque proietta il raggio associato a quel pixel. La scelta di questo valore è data dal voler ottenere una dimensione dei blocchi multipla di $32$ per utilizzare al massimo ogni warp, ma anche di voler ridurre al minimo la dimensione dei blocchi per ridurre al minimo la divergenza nel warp.\n",
        "\n",
        "Il calcolo di due raggi può essere significativamente diverso in base al materiale che intersecano: determinare il colore di un raggio che non colpisce nulla ha un costo computazionale nullo, invece determinare il colore di un raggio che colpisce un materiale altamente riflettente implica dover calcolare altri raggi riflessi. Questo vuol dire che due raggi proiettati da thread all'interno dello stesso blocco possono dover eseguire algoritmi estremamente diversi, il che implica un problema significativo di divergenza nel warp.\n",
        "\n",
        "Ridurre la dimensione dei blocchi non aiuta a ridurre la divergenza nel calcolo dei singoli raggi, ma nella pratica raggi vicini vanno a collidere con lo stesso materiale e dunque eseguono lo stesso algoritmo, **il limite tra cazzata e genio è molto sottile qui, bisogna stare attenti a quello che si scrive**\n",
        "\n",
        "    TABELLA DELLE PERFORMANCE VARIANDO blockSize  \n",
        "    mostra (4, 4), (8, 4), (8, 8), (16, 8), (16, 16)\n",
        "\n",
        "I thread scrivono il colore finale del loro pixel su un framebuffer allocato utilizzando la managed memory: il motivo di questa scelta è il fatto che la memoria del framebuffer viene spostata tra host e device solamente all'inizio ed al termine del rendering di un fotogramma, dunque seppur non sia complicato gestire questi spostamenti manualmente non ho trovato alcuna differenza tra i due modi.\n",
        "\n",
        "Prima di iniziare il rendering vengono caricate in *constant memory* gli elementi della scena: questi includono gli oggetti e i loro materiali associati. Infine i valori *RGB* delle texture utilizzate dagli oggetti sono salvati in *texture memory*."
      ],
      "metadata": {
        "id": "ALkrSKwodnpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confronto con implementazione CPU"
      ],
      "metadata": {
        "id": "-w32CubTwNYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La versione del raytracer accelerato via CUDA offre un notevole speedup rispetto a quelle implementate su CPU, sia che utilizzino un singolo thread sia molteplici.\n",
        "\n",
        "Le implementazioni CPU-based confrontate sono quella ufficiale del libro[2], utilizzante un solo thread, ed una mia implementazione[3] scritta in passato che utilizza un numero di thread pari al numero di core della CPU.\n",
        "Questo confronto non pretende di essere un confronto particolarmente corretto nei confronti di quest'ultime due implementazioni dato che quella del libro esiste solamente a scopo didattico, e dunque non si concentra sulle performance, mentre invece la mia implementazione CPU-based è scritta nel linguaggio di programmazione Rust che, seppur comunque nello stesso range di prestazioni del C++, è comunque un linguaggio diverso.\n",
        "\n",
        "Le implementazioni CUDA e C++ sono state compilate utilizzando il flag `-O3`, la implementazione in Rust è stata compilata con il flag `--build release`: queste flag dovrebbero aver abilitato il massimo livello di ottimizzazione.\n",
        "\n",
        "Il confronto è stato fatto eseguendo ogni programma 10 volte, una dopo l'altra, renderizzando una immagine di risoluzione 8k (7680 x 4320 pixels) della scena mostrata nel video ad inizio relazione. La macchina usata nei test è una macchina offerta da *Colab* con le seguenti specifiche:\n",
        "\n",
        "    CPU: Intel X\n",
        "    RAM: Y GB\n",
        "    GPU: NVidia Z\n",
        "    OS: Linux 64bit\n",
        "\n",
        "I tempi misurati sono solamente quelli per il rendering dell'immagine, definito come il momento in cui il framebuffer in memoria contiene i valori RGB di tutti i pixel. Queste misurazioni dunque escludono il tempo speso per preparare la scena da renderizzare e per scrivere l'immagine finale su disco.\n",
        "\n",
        "I risultati finali, mostrati nella tabella qui sottostante, mostrano uno speedup schiacciante rispetto alle versioni CPU-based; la versione CUDA è in media 80000000000% più veloce di quella CPU-based a single thread e in media 60000000000000% più veloce di quella multi-thread.\n",
        "\n",
        "| Test \t| 1 \t| 2 \t| 3 \t| 4 \t| 5 \t| 6 \t| 7 \t| 8 \t| 9 \t| 10 \t| Media \t|\n",
        "|---------------\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|---\t|----\t|-------\t|\n",
        "| CUDA          \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|    \t|       \t|\n",
        "| 1-Thread      \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|    \t|       \t|\n",
        "| 8-Thread      \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|   \t|    \t|       \t|\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zmvN7hgdp5OD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profiling delle prestazioni"
      ],
      "metadata": {
        "id": "1z3-Qgaawphp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**parlo un po' dei risultati di nv-nsight-cu-cli e dei bottleneck**"
      ],
      "metadata": {
        "id": "yh8NRLZ_wqbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Showcase di immagini generate\n"
      ],
      "metadata": {
        "id": "EB8TKL4YciIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Inserire immagine delle tre sfere con i materiali\n",
        "\n",
        "    Inserire immagine delle tante sfere random dal libro\n",
        "\n",
        "    Inserire immagine delle sfere con texture pianeti"
      ],
      "metadata": {
        "id": "WOrwPM9Du-r9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliografia\n",
        "[1]: [_Ray Tracing in One Weekend_](https://raytracing.github.io/books/RayTracingInOneWeekend.html), Peter Shirley, 2020  \n",
        "[2]: [_Ray Tracing in One Weekend Source Code_](https://github.com/RayTracing/raytracing.github.io/tree/master/src/InOneWeekend), Peter Shirley, 2020  \n",
        "[3]: [Sphere Point Picking](https://mathworld.wolfram.com/SpherePointPicking.html), Eric W. Weisstein, Wolfram MathWorld"
      ],
      "metadata": {
        "id": "hMRjL6u8aKiG"
      }
    }
  ]
}